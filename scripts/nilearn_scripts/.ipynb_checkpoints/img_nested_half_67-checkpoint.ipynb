{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nilearn\n",
    "import glob\n",
    "#import matplotlib\n",
    "import nibabel as nib\n",
    "from nilearn.image import concat_imgs, index_img, smooth_img\n",
    "from nilearn.image import resample_to_img\n",
    "#from nilearn import plotting\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score, permutation_test_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from nilearn import image\n",
    "#from nilearn.plotting import plot_stat_map, show\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import LeaveOneLabelOut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---STEP 1---\n",
    "#### expects concatenated image\n",
    "#Note: the data does not fit into a NIFTI-1 file, due to large number of subs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setPaths():\n",
    "Set directory and file paths. We assume we have a concatenated image.  \n",
    "We load and average anat image, a mask image, and a label file(csv). \n",
    "__________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setPaths():\n",
    "    global basepath\n",
    "    global outpath\n",
    "    global fmri_subjs\n",
    "    global average_ana\n",
    "    global imag_mask\n",
    "    global stim\n",
    "    \n",
    "    #set basepath\n",
    "    basepath=os.path.join('/projects','niblab','data','eric_data','W1','imagine')\n",
    "    outpath = \"/projects/niblab/nilearn_projects\"\n",
    "# ---STEP 2---\n",
    "#load & prepare MRI data\n",
    "#load, fxnl, anatomical, & mask for plotting\n",
    "    fmri_subjs=os.path.join(basepath, 'concatenated_imagine.nii')\n",
    "    average_ana=os.path.join(basepath,'CS_avg_mprage_image.nii.gz')\n",
    "    imag_mask=os.path.join(outpath,'power_roimask_4bi.nii.gz')\n",
    "    stim = os.path.join('/projects','niblab','scripts','nilean_stuff','label_67_sub.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData():\n",
    "    \n",
    "\n",
    "#load labels for the functional data\n",
    "    labels = np.recfromcsv(stim, delimiter=\",\",encoding='UTF-8')\n",
    "    #print(labels)\n",
    "#Its shape corresponds to the number of time-points times the number of voxels in the mask\n",
    "    func_df = pd.read_csv(stim, sep=\",\")\n",
    "#Retrieve the behavioral targets, that we are going to predict in the decoding\n",
    "    #y_mask = labels['labels']\n",
    "    #subs = labels['subs']\n",
    "    y_mask =  func_df['labels']\n",
    "    subs = func_df['subs']\n",
    "\n",
    "    return y_mask, subs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---STEP 3---\n",
    "### feature selection\n",
    "To keep only data corresponding to app food or unapp food, we create a mask of the samples belonging to the condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection():\n",
    "    #condition_mask = np.logical_or(y_mask == b'app',y_mask == b'unapp')\n",
    "    condition_mask = func_df[\"labels\"].isin(['app', 'unapp'])\n",
    "    print(condition_mask.shape)\n",
    "    #y = y_mask[condition_mask]\n",
    "    y = y_mask[condition_mask]\n",
    "    print(y.shape)\n",
    "    n_conditions = np.size(np.unique(y))\n",
    "    print(n_conditions)\n",
    "#n_conditions = np.size(np.unique(y))\n",
    "    \n",
    "    #session = func_df[condition_mask].to_records(index=False)\n",
    "    #print(session.dtype.name)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "#prepare the fxnl data.\n",
    "    nifti_masker = NiftiMasker(mask_img=imag_mask,\n",
    "                           smoothing_fwhm=4,standardize=True,\n",
    "                           memory_level=0)\n",
    "\n",
    "    fmri_trans = nifti_masker.fit_transform(fmri_subjs)\n",
    "    print(fmri_trans)\n",
    "    X = fmri_trans[condition_mask]\n",
    "    subs = subs[condition_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---STEP 4---\n",
    "setting prediction  & testing the classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---STEP 4---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setPrediction():\n",
    "        \n",
    "    svc = SVC(kernel='linear')\n",
    "    print(svc)\n",
    "\n",
    "# Define the dimension reduction to be used.\n",
    "# Here we use a classical univariate feature selection based on F-test, namely Anova. We set the number of features to be selected to 500\n",
    "    feature_selection = SelectKBest(f_classif, k=500)\n",
    "\n",
    "# We have our classifier (SVC), our feature selection (SelectKBest), and now, we can plug them together in a *pipeline* that performs the two operations successively:\n",
    "    anova_svc = Pipeline([('anova',feature_selection), ('svc',svc)])\n",
    "\n",
    "#fit the decoder and predict\n",
    "    anova_svc.fit(X, y)\n",
    "    y_pred = anova_svc.predict(X)\n",
    "\n",
    "\n",
    "    cv = LeaveOneLabelOut(subs[subs < 1])\n",
    "    k_range = [10, 15, 30, 50 , 150, 300, 500, 1000, 1500, 3000, 5000]\n",
    "    cv_scores = cross_val_score(anova_svc, X[subs ==1], y[subs ==1])\n",
    "\n",
    "    scores_validation = []\n",
    "    cv_means =[]\n",
    "\n",
    "# we are working with a composite estimator:\n",
    "# a pipeline of feature selection followed by SVC. Thus to give the name of the parameter that we want to tune we need to give the name of the step in\n",
    "# the pipeline, followed by the name of the parameter, with ‘__’ as a separator.\n",
    "# We are going to tune the parameter 'k' of the step called 'anova' in the pipeline. Thus we need to address it as 'anova__k'.\n",
    "# Note that GridSearchCV takes an n_jobs argument that can make it go much faster\n",
    "    grid = GridSearchCV(anova_svc, param_grid={'anova__k': k_range},n_jobs=-1)\n",
    "    nested_cv_scores = cross_val_score(grid, X, y)\n",
    "    classification_accuracy = np.mean(nested_cv_scores)\n",
    "    print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "          (classification_accuracy, 1. / n_conditions))\n",
    "\n",
    "\n",
    "    for k in k_range:\n",
    "        curr_k = k\n",
    "        anova_svc.set_params(anova__k=curr_k, svc__C=1.0).fit(X[subs == 1], y[subs == 1])\n",
    "        cv_scores = cross_val_score(anova_svc, X[subs ==1], y[subs ==1])\n",
    "        cv_means.append(cv_scores.mean())\n",
    "        print(\"CV score: %.4f\" % cv_scores[-1])\n",
    "        y_pred = anova_svc.predict(X[subs == 0])\n",
    "        scores_validation.append(np.mean(y_pred == y[subs == 0]))\n",
    "        print(\"score validation: %.4f\" % scores_validation[-1])\n",
    "\n",
    "    print(\"SCORE VALIDATION: \", scores_validation)\n",
    "    print(\"CV MEANS: \", cv_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
